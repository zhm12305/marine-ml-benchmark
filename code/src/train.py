import argparse, joblib, optuna, numpy as np, pandas as pd
from pathlib import Path
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from src.utils_io import read_cfg

ROOT = Path(__file__).resolve().parents[1]

# ---------------- model registry ----------------
def get_model(name, params):
    if name == "rf":
        return RandomForestRegressor(random_state=0, **params)
    if name == "xgb":
        return XGBRegressor(random_state=0, **params)
    if name == "svr":
        return SVR(**params)
    raise ValueError(name)

search_space = {
    "rf":  {"n_estimators": (50, 200), "max_depth": (5, 15)},  # å‡å°‘èŒƒå›´åŠ å¿«è®­ç»ƒ
    "xgb": {"learning_rate": (0.05, 0.2), "n_estimators": (100, 300),
            "max_depth": (3, 8)},
    "svr": {"C": (0.1, 10), "gamma": (1e-3, 1e-1)}
}

# ------------------------- main ------------------
def objective(trial, X, y, model_name):
    params = {}
    for k, rng in search_space[model_name].items():
        if k in ['n_estimators', 'max_depth']:
            params[k] = trial.suggest_int(k, *rng)
        else:
            params[k] = trial.suggest_float(k, *rng, log=True)
    model  = make_pipeline(StandardScaler(), get_model(model_name, params))
    cv     = TimeSeriesSplit(n_splits=5)
    scores = []
    for tr, te in cv.split(X):
        model.fit(X[tr], y[tr])
        scores.append(model.score(X[te], y[te]))
    return -np.mean(scores)

def train_dataset(ds_key, model_name, cfg):
    print(f"ğŸ¤– è®­ç»ƒ {ds_key} - {model_name}")

    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    data_path = ROOT / "data_proc" / ds_key / "clean.csv"
    if not data_path.exists():
        print(f"   âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
        return None

    df = pd.read_csv(data_path)
    tcol = cfg["datasets"][ds_key].get("target_col")
    if not tcol:
        print(f"   âŒ æ— ç›®æ ‡åˆ—")
        return None

    if tcol not in df.columns:
        # å°è¯•æŸ¥æ‰¾ç›¸ä¼¼çš„ç›®æ ‡åˆ—
        possible_cols = [col for col in df.columns if any(keyword in col.lower()
                        for keyword in ['chla', 'chlorophyll', 'target', 'value', 'abundance'])]
        if possible_cols:
            tcol = possible_cols[0]
            print(f"   âš ï¸  ä½¿ç”¨ {tcol} æ›¿ä»£åŸç›®æ ‡åˆ—")
        else:
            print(f"   âŒ ç›®æ ‡åˆ— {tcol} ä¸å­˜åœ¨")
            return None

    X = df.drop(columns=[tcol]).select_dtypes(include=[np.number]).to_numpy()
    y = df[tcol].to_numpy()

    if len(X) == 0 or len(y) == 0:
        print(f"   âŒ æ•°æ®ä¸ºç©º")
        return None

    print(f"   ğŸ“Š æ•°æ®: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
    print(f"   ğŸ” å¼€å§‹è¶…å‚æ•°æœç´¢ ({cfg['global']['n_trials']} æ¬¡è¯•éªŒ)...")

    study = optuna.create_study(
        direction='maximize',  # æœ€å¤§åŒ–RÂ²
        sampler=optuna.samplers.TPESampler(seed=cfg["global"]["random_seed"])
    )

    # ä¿®æ”¹objectiveå‡½æ•°è¿”å›æ­£å€¼ï¼ˆRÂ²ï¼‰
    def modified_objective(trial):
        return -objective(trial, X, y, model_name)  # è½¬æ¢ä¸ºæ­£å€¼

    study.optimize(modified_objective, n_trials=cfg["global"]["n_trials"],
                   show_progress_bar=True)

    print(f"   âœ… æœ€ä½³ RÂ²: {study.best_value:.4f}")

    best_model = make_pipeline(StandardScaler(),
                               get_model(model_name, study.best_params))
    best_model.fit(X, y)
    out_dir = ROOT / "models" / ds_key
    out_dir.mkdir(parents=True, exist_ok=True)
    joblib.dump(best_model, out_dir / f"{model_name}.pkl")

    return {"dataset": ds_key, "model": model_name,
            "best_params": study.best_params, "score": study.best_value}

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", default="all")
    parser.add_argument("--model",  default="all")
    args = parser.parse_args()

    cfg = read_cfg()
    dsets = cfg["datasets"].keys() if args.dataset == "all" else [args.dataset]
    models = ["rf", "xgb", "svr"] if args.model == "all" else [args.model]

    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ {len(dsets)} ä¸ªæ•°æ®é›† Ã— {len(models)} ä¸ªæ¨¡å‹")
    print("=" * 60)

    records = []
    total_combinations = sum(1 for d in dsets if cfg["datasets"][d].get("target_col")) * len(models)
    current = 0

    for d in dsets:
        if not cfg["datasets"][d].get("target_col"):
            print(f"â­ï¸  è·³è¿‡ {d}: æ— ç›®æ ‡åˆ—")
            continue

        for m in models:
            current += 1
            print(f"\n[{current}/{total_combinations}] å½“å‰ç»„åˆ: {d} + {m}")
            print("-" * 40)

            try:
                rec = train_dataset(d, m, cfg)
                if rec:
                    records.append(rec)
                    print(f"   âœ… å®Œæˆ: RÂ² = {rec['score']:.4f}")
                else:
                    print(f"   âŒ å¤±è´¥: è¿”å›ç©ºç»“æœ")
            except KeyboardInterrupt:
                print(f"\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
                break
            except Exception as e:
                print(f"   âŒ å¤±è´¥: {e}")
                continue
        else:
            continue  # åªæœ‰åœ¨å†…å±‚å¾ªç¯æ­£å¸¸å®Œæˆæ—¶æ‰ç»§ç»­
        break  # å¦‚æœå†…å±‚å¾ªç¯è¢«breakï¼Œå¤–å±‚ä¹Ÿbreak

    # ä¿å­˜ç»“æœ
    if records:
        results_df = pd.DataFrame(records)
        results_df.to_csv(ROOT / "tables" / "train_log.csv", index=False)
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"âœ… æˆåŠŸè®­ç»ƒ: {len(records)} ä¸ªæ¨¡å‹")
        print(f"ğŸ“ ç»“æœä¿å­˜è‡³: tables/train_log.csv")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜è‡³: models/")

        # æ˜¾ç¤ºæœ€ä½³ç»“æœ
        if len(records) > 0:
            best_result = max(records, key=lambda x: x['score'])
            print(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_result['dataset']} + {best_result['model']} (RÂ² = {best_result['score']:.4f})")
    else:
        print(f"\nâŒ æ²¡æœ‰æˆåŠŸè®­ç»ƒçš„æ¨¡å‹")
