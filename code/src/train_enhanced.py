import argparse, joblib, optuna, numpy as np, pandas as pd
import json, torch, torch.nn as nn
from pathlib import Path
from sklearn.model_selection import TimeSeriesSplit, train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.pipeline import make_pipeline
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
from src.utils_io import read_cfg
import warnings
warnings.filterwarnings('ignore')

ROOT = Path(__file__).resolve().parents[1]

# è®¾ç½®è®¾å¤‡ï¼ˆGPUä¼˜å…ˆï¼‰
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory // 1024**3}GB")

def calculate_metrics(y_true, y_pred):
    """è®¡ç®—å¤šä¸ªè¯„ä¼°æŒ‡æ ‡"""
    metrics = {}

    # RÂ² Score
    metrics['R2'] = r2_score(y_true, y_pred)

    # Mean Absolute Error
    metrics['MAE'] = mean_absolute_error(y_true, y_pred)

    # Root Mean Square Error
    metrics['RMSE'] = mean_squared_error(y_true, y_pred, squared=False)

    # Mean Absolute Percentage Error (å¤„ç†é™¤é›¶é—®é¢˜)
    try:
        # é¿å…é™¤é›¶ï¼Œåªåœ¨ç›®æ ‡å€¼ä¸ä¸º0æ—¶è®¡ç®—MAPE
        non_zero_mask = y_true != 0
        if non_zero_mask.sum() > 0:
            metrics['MAPE'] = mean_absolute_percentage_error(y_true[non_zero_mask], y_pred[non_zero_mask])
        else:
            metrics['MAPE'] = float('inf')  # æ‰€æœ‰çœŸå®å€¼éƒ½ä¸º0
    except:
        metrics['MAPE'] = float('inf')

    # ç›¸å…³ç³»æ•°
    try:
        correlation = np.corrcoef(y_true, y_pred)[0, 1]
        metrics['Correlation'] = correlation if not np.isnan(correlation) else 0.0
    except:
        metrics['Correlation'] = 0.0

    # æ£€æŸ¥æŒ‡æ ‡æœ‰æ•ˆæ€§
    for key, value in metrics.items():
        if np.isnan(value) or np.isinf(value):
            if key == 'MAPE':
                metrics[key] = 999.0  # MAPEçš„é»˜è®¤å¤§å€¼
            else:
                metrics[key] = 0.0 if key in ['R2', 'Correlation'] else 999.0

    return metrics

# ================ æ·±åº¦å­¦ä¹ æ¨¡å‹å®šä¹‰ ================
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        out, _ = self.lstm(x, (h0, c0))
        out = self.dropout(out[:, -1, :])  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        out = self.fc(out)
        return out

class TransformerModel(nn.Module):
    def __init__(self, input_size, d_model=64, nhead=4, num_layers=2, dropout=0.1):
        super(TransformerModel, self).__init__()

        # é™ç»´å¤„ç†ï¼šå…ˆå°†é«˜ç»´ç‰¹å¾é™åˆ°åˆç†èŒƒå›´
        self.feature_reduction = nn.Linear(input_size, min(32, input_size//2))
        self.input_projection = nn.Linear(min(32, input_size//2), d_model)

        # ç®€åŒ–ä½ç½®ç¼–ç 
        self.pos_encoding = nn.Parameter(torch.randn(100, d_model) * 0.1)

        # ä½¿ç”¨æ›´å°çš„Transformer
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model*2,
            dropout=dropout,
            batch_first=True,
            activation='gelu'
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # è¾“å‡ºå±‚
        self.layer_norm = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Sequential(
            nn.Linear(d_model, d_model//2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model//2, 1)
        )

        # æƒé‡åˆå§‹åŒ–
        self._init_weights()

    def _init_weights(self):
        """æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)

    def forward(self, x):
        batch_size, seq_len, _ = x.size()

        # ç‰¹å¾é™ç»´
        x = self.feature_reduction(x)
        x = torch.relu(x)

        # è¾“å…¥æŠ•å½±
        x = self.input_projection(x)

        # ä½ç½®ç¼–ç  - ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        pos_enc = self.pos_encoding[:seq_len, :].unsqueeze(0).expand(batch_size, -1, -1)
        x = x + pos_enc

        # Transformerç¼–ç 
        x = self.transformer(x)

        # å…¨å±€å¹³å‡æ± åŒ– + å±‚å½’ä¸€åŒ–
        x = self.layer_norm(x.mean(dim=1))
        x = self.dropout(x)

        # è¾“å‡º
        x = self.fc(x)
        return x

# ================ æ¨¡å‹æ³¨å†Œè¡¨ ================
def get_model(name, params, input_size=None):
    if name == "rf":
        return RandomForestRegressor(random_state=42, **params)
    elif name == "xgb":
        return XGBRegressor(random_state=42, **params)
    elif name == "svr":
        return SVR(**params)
    elif name == "lstm":
        return LSTMModel(input_size=input_size, **params)
    elif name == "transformer":
        return TransformerModel(input_size=input_size, **params)
    else:
        raise ValueError(f"Unknown model: {name}")

def get_search_space(model_name, cfg):
    """ä»é…ç½®æ–‡ä»¶è·å–æœç´¢ç©ºé—´"""
    if model_name not in cfg["global"]["models"]:
        raise ValueError(f"Model {model_name} not configured")

    # å°†åˆ—è¡¨è½¬æ¢ä¸ºå…ƒç»„
    search_space = cfg["global"]["models"][model_name]
    return {param: tuple(range_list) for param, range_list in search_space.items()}

# ================ è®­ç»ƒå‡½æ•° ================
def train_sklearn_model(X, y, model_name, cfg):
    """è®­ç»ƒsklearnæ¨¡å‹"""
    search_space = get_search_space(model_name, cfg)
    
    def objective(trial):
        params = {}
        for param, (low, high) in search_space.items():
            if isinstance(low, int) and isinstance(high, int):
                params[param] = trial.suggest_int(param, low, high)
            elif param in ['learning_rate', 'gamma']:
                # å¯¹æ•°å°ºåº¦å‚æ•°
                params[param] = trial.suggest_float(param, low, high, log=True)
            else:
                # çº¿æ€§å°ºåº¦å‚æ•°
                params[param] = trial.suggest_float(param, low, high, log=False)
        
        model = make_pipeline(StandardScaler(), get_model(model_name, params))
        cv = TimeSeriesSplit(n_splits=cfg["global"]["cv_folds"])
        scores = []
        
        for train_idx, val_idx in cv.split(X):
            model.fit(X[train_idx], y[train_idx])
            pred = model.predict(X[val_idx])
            scores.append(r2_score(y[val_idx], pred))
        
        return np.mean(scores)
    
    study = optuna.create_study(direction='maximize', 
                               sampler=optuna.samplers.TPESampler(seed=cfg["global"]["random_seed"]))
    study.optimize(objective, n_trials=cfg["global"]["n_trials"], show_progress_bar=False)
    
    # ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹å¹¶è®¡ç®—å¤šä¸ªæŒ‡æ ‡
    best_model = make_pipeline(StandardScaler(), get_model(model_name, study.best_params))

    # ä½¿ç”¨è®­ç»ƒ-æµ‹è¯•åˆ†å‰²æ¥è¯„ä¼°æœ€ç»ˆæ¨¡å‹
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=cfg["global"]["random_seed"])

    best_model.fit(X_train, y_train)
    y_pred = best_model.predict(X_test)

    # è®¡ç®—å¤šä¸ªè¯„ä¼°æŒ‡æ ‡
    metrics = calculate_metrics(y_test, y_pred)

    # åœ¨å…¨éƒ¨æ•°æ®ä¸Šé‡æ–°è®­ç»ƒç”¨äºä¿å­˜
    best_model.fit(X, y)

    return best_model, study.best_params, study.best_value, metrics

def train_deep_model(X, y, model_name, cfg):
    """è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹"""
    # æ•°æ®åˆ†å‰²
    train_ratio = cfg["global"]["train_ratio"]
    val_ratio = cfg["global"]["val_ratio"]
    
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=1-train_ratio-val_ratio, random_state=cfg["global"]["random_seed"]
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=val_ratio/(train_ratio+val_ratio), 
        random_state=cfg["global"]["random_seed"]
    )
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler_X = MinMaxScaler()
    scaler_y = StandardScaler()
    
    X_train_scaled = scaler_X.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
    X_val_scaled = scaler_X.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)
    
    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
    y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).flatten()
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶ç§»åŠ¨åˆ°GPU
    X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)
    y_train_tensor = torch.FloatTensor(y_train_scaled).to(device)
    X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)
    y_val_tensor = torch.FloatTensor(y_val_scaled).to(device)
    
    search_space = get_search_space(model_name, cfg)
    
    def objective(trial):
        try:
            params = {}
            for param, (low, high) in search_space.items():
                if param == "learning_rate":
                    params[param] = trial.suggest_float(param, low, high, log=True)
                elif isinstance(low, int):
                    params[param] = trial.suggest_int(param, low, high)
                else:
                    params[param] = trial.suggest_float(param, low, high, log=False)

            lr = params.pop("learning_rate", 1e-3)
            model = get_model(model_name, params, input_size=X_train_scaled.shape[-1]).to(device)

            # æ·»åŠ æƒé‡è¡°å‡å’Œå­¦ä¹ ç‡è°ƒåº¦
            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)
            criterion = nn.MSELoss()
            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer, mode='min', factor=0.5, patience=3, verbose=False
            )
        
            # æ”¹è¿›çš„è®­ç»ƒå¾ªç¯
            model.train()
            best_val_loss = float('inf')
            patience = 5
            patience_counter = 0

            for epoch in range(50):  # å‡å°‘epochæ•°ä»¥åŠ å¿«é€Ÿåº¦
                # è®­ç»ƒæ­¥éª¤
                optimizer.zero_grad()
                outputs = model(X_train_tensor).squeeze()
                train_loss = criterion(outputs, y_train_tensor)

                # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºNaN
                if torch.isnan(train_loss):
                    return 0.0  # è¿”å›æœ€å·®åˆ†æ•°

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                train_loss.backward()
                optimizer.step()

                # éªŒè¯ï¼ˆæ¯5ä¸ªepochï¼‰
                if epoch % 5 == 0:
                    model.eval()
                    with torch.no_grad():
                        val_outputs = model(X_val_tensor).squeeze()
                        val_loss = criterion(val_outputs, y_val_tensor)

                        # æ£€æŸ¥éªŒè¯æŸå¤±
                        if torch.isnan(val_loss):
                            return 0.0

                        # æ—©åœæ£€æŸ¥
                        if val_loss < best_val_loss:
                            best_val_loss = val_loss
                            patience_counter = 0
                        else:
                            patience_counter += 1

                        if patience_counter >= patience:
                            break

                        # å­¦ä¹ ç‡è°ƒåº¦
                        scheduler.step(val_loss)

                    model.train()

            # æœ€ç»ˆéªŒè¯
            model.eval()
            with torch.no_grad():
                val_outputs = model(X_val_tensor).squeeze()
                val_pred = scaler_y.inverse_transform(val_outputs.cpu().numpy().reshape(-1, 1)).flatten()
                val_true = scaler_y.inverse_transform(y_val_scaled.reshape(-1, 1)).flatten()
                score = r2_score(val_true, val_pred)

                # ç¡®ä¿è¿”å›æœ‰æ•ˆåˆ†æ•°
                if np.isnan(score) or np.isinf(score):
                    return 0.0

                return max(0.0, score)  # ç¡®ä¿åˆ†æ•°éè´Ÿ

        except Exception as e:
            print(f"Trial failed: {e}")
            return 0.0  # è¿”å›æœ€å·®åˆ†æ•°
    
    study = optuna.create_study(direction='maximize',
                               sampler=optuna.samplers.TPESampler(seed=cfg["global"]["random_seed"]))
    study.optimize(objective, n_trials=cfg["global"]["n_trials"]//2, show_progress_bar=False)  # å‡å°‘è¯•éªŒæ¬¡æ•°
    
    # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    best_params = study.best_params.copy()
    lr = best_params.pop("learning_rate", 1e-3)
    final_model = get_model(model_name, best_params, input_size=X_train_scaled.shape[-1]).to(device)
    
    optimizer = torch.optim.Adam(final_model.parameters(), lr=lr)
    criterion = nn.MSELoss()
    
    # åˆå¹¶è®­ç»ƒå’ŒéªŒè¯é›†è¿›è¡Œæœ€ç»ˆè®­ç»ƒ
    X_final = np.concatenate([X_train_scaled, X_val_scaled])
    y_final = np.concatenate([y_train_scaled, y_val_scaled])
    X_final_tensor = torch.FloatTensor(X_final).to(device)
    y_final_tensor = torch.FloatTensor(y_final).to(device)
    
    final_model.train()
    for epoch in range(200):
        optimizer.zero_grad()
        outputs = final_model(X_final_tensor).squeeze()
        loss = criterion(outputs, y_final_tensor)
        loss.backward()
        optimizer.step()
    
    # åŒ…è£…æ¨¡å‹ä»¥ä¾¿ä¿å­˜
    class DeepModelWrapper:
        def __init__(self, model, scaler_X, scaler_y):
            self.model = model
            self.scaler_X = scaler_X
            self.scaler_y = scaler_y
            
        def predict(self, X):
            self.model.eval()
            X_scaled = self.scaler_X.transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)
            X_tensor = torch.FloatTensor(X_scaled)
            with torch.no_grad():
                outputs = self.model(X_tensor).squeeze()
                predictions = self.scaler_y.inverse_transform(outputs.numpy().reshape(-1, 1)).flatten()
            return predictions
    
    wrapped_model = DeepModelWrapper(final_model, scaler_X, scaler_y)

    # è®¡ç®—æœ€ç»ˆæ¨¡å‹çš„å¤šä¸ªè¯„ä¼°æŒ‡æ ‡
    final_model.eval()
    with torch.no_grad():
        # ä½¿ç”¨éªŒè¯é›†è¯„ä¼°
        val_outputs = final_model(X_val_tensor).squeeze()
        val_pred = scaler_y.inverse_transform(val_outputs.cpu().numpy().reshape(-1, 1)).flatten()
        val_true = scaler_y.inverse_transform(y_val_scaled.reshape(-1, 1)).flatten()

        # è®¡ç®—å¤šä¸ªæŒ‡æ ‡
        metrics = calculate_metrics(val_true, val_pred)

    return wrapped_model, study.best_params, study.best_value, metrics

def train_dataset(ds_key, model_name, cfg):
    """è®­ç»ƒå•ä¸ªæ•°æ®é›†çš„æ¨¡å‹"""
    print(f"è®­ç»ƒ {ds_key} - {model_name}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç›®æ ‡åˆ—
    if not cfg["datasets"][ds_key].get("target_col"):
        print(f"  è·³è¿‡ {ds_key}: æ— ç›®æ ‡åˆ—")
        return None
    
    # åŠ è½½æ•°æ®
    data_path = ROOT / "data_proc" / ds_key / "clean.csv"
    if not data_path.exists():
        print(f"  è·³è¿‡ {ds_key}: æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
        return None
    
    df = pd.read_csv(data_path)
    target_col = cfg["datasets"][ds_key]["target_col"]
    
    if model_name in ["lstm", "transformer"]:
        # æ·±åº¦å­¦ä¹ æ¨¡å‹ä½¿ç”¨åºåˆ—æ•°æ®
        seq_path = ROOT / "data_proc" / ds_key / "sequences.npz"
        if not seq_path.exists():
            print(f"  è·³è¿‡ {ds_key}: åºåˆ—æ•°æ®ä¸å­˜åœ¨")
            return None
        
        data = np.load(seq_path)
        X, y = data['X'], data['y']
        
        if len(X) == 0:
            print(f"  è·³è¿‡ {ds_key}: åºåˆ—æ•°æ®ä¸ºç©º")
            return None
        
        model, best_params, best_score, metrics = train_deep_model(X, y, model_name, cfg)
    else:
        # ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹
        if target_col not in df.columns:
            print(f"  è·³è¿‡ {ds_key}: ç›®æ ‡åˆ— {target_col} ä¸å­˜åœ¨")
            return None

        X = df.drop(columns=[target_col]).select_dtypes(include=[np.number]).values
        y = df[target_col].values

        if len(X) == 0 or len(y) == 0:
            print(f"  è·³è¿‡ {ds_key}: æ•°æ®ä¸ºç©º")
            return None

        model, best_params, best_score, metrics = train_sklearn_model(X, y, model_name, cfg)
    
    # ä¿å­˜æ¨¡å‹
    out_dir = ROOT / "models" / ds_key
    out_dir.mkdir(parents=True, exist_ok=True)
    
    if model_name in ["lstm", "transformer"]:
        # æ·±åº¦å­¦ä¹ æ¨¡å‹éœ€è¦ç‰¹æ®Šä¿å­˜æ–¹å¼
        torch.save({
            'model_state_dict': model.model.state_dict(),
            'scaler_X': model.scaler_X,
            'scaler_y': model.scaler_y,
            'model_params': best_params
        }, out_dir / f"{model_name}.pth")
    else:
        joblib.dump(model, out_dir / f"{model_name}.pkl")
    
    # ä¿å­˜æœ€ä½³å‚æ•°
    with open(out_dir / f"{model_name}_params.json", 'w') as f:
        json.dump(best_params, f, indent=2)
    
    print(f"  å®Œæˆ {ds_key} - {model_name}: RÂ² = {best_score:.4f}")
    
    # æ„å»ºç»“æœå­—å…¸ï¼ŒåŒ…å«å¤šä¸ªè¯„ä¼°æŒ‡æ ‡
    result = {
        "dataset": ds_key,
        "model": model_name,
        "best_params": best_params,
        "best_score": best_score,
        "data_shape": X.shape if 'X' in locals() else "N/A"
    }

    # æ·»åŠ è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡
    result.update(metrics)

    return result

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", default="all", help="Dataset to train on")
    parser.add_argument("--model", default="all", help="Model to train")
    args = parser.parse_args()
    
    cfg = read_cfg()
    
    # ç¡®å®šæ•°æ®é›†
    if args.dataset == "all":
        datasets = list(cfg["datasets"].keys())
    else:
        datasets = [args.dataset]
    
    # ç¡®å®šæ¨¡å‹
    if args.model == "all":
        models = ["rf", "xgb", "svr", "lstm", "transformer"]
    else:
        models = [args.model]
    
    # è®­ç»ƒæ‰€æœ‰ç»„åˆ
    results = []
    for dataset in datasets:
        for model in models:
            try:
                result = train_dataset(dataset, model, cfg)
                if result:
                    results.append(result)
            except Exception as e:
                print(f"è®­ç»ƒå¤±è´¥ {dataset} - {model}: {e}")
    
    # ä¿å­˜è®­ç»ƒæ—¥å¿— - ä¿®å¤è¿½åŠ é€»è¾‘
    if results:
        results_df = pd.DataFrame(results)
        log_file = ROOT / "tables" / "train_log_enhanced.csv"

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if log_file.exists():
            # è¯»å–ç°æœ‰æ•°æ®
            try:
                existing_df = pd.read_csv(log_file)
                # åˆå¹¶æ•°æ®ï¼Œé¿å…é‡å¤
                combined_df = pd.concat([existing_df, results_df], ignore_index=True)
                # å»é™¤å¯èƒ½çš„é‡å¤è¡Œï¼ˆåŸºäºdatasetå’Œmodelï¼‰
                combined_df = combined_df.drop_duplicates(subset=['dataset', 'model'], keep='last')
                combined_df.to_csv(log_file, index=False)
                print(f"\nâœ“ è®­ç»ƒå®Œæˆï¼Œå…± {len(results)} ä¸ªæ–°æ¨¡å‹")
                print(f"å·²è¿½åŠ åˆ°ç°æœ‰æ—¥å¿—ï¼Œæ€»è®¡ {len(combined_df)} ä¸ªæ¨¡å‹")
            except Exception as e:
                print(f"è¯»å–ç°æœ‰æ—¥å¿—å¤±è´¥: {e}")
                # å¦‚æœè¯»å–å¤±è´¥ï¼Œç›´æ¥è¦†ç›–
                results_df.to_csv(log_file, index=False)
                print(f"\nâœ“ è®­ç»ƒå®Œæˆï¼Œå…± {len(results)} ä¸ªæ¨¡å‹")
        else:
            # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶
            log_file.parent.mkdir(parents=True, exist_ok=True)
            results_df.to_csv(log_file, index=False)
            print(f"\nâœ“ è®­ç»ƒå®Œæˆï¼Œå…± {len(results)} ä¸ªæ¨¡å‹")

        print("è¯¦ç»†ç»“æœä¿å­˜è‡³ tables/train_log_enhanced.csv")
    else:
        print("æ²¡æœ‰æˆåŠŸè®­ç»ƒçš„æ¨¡å‹")
