#!/usr/bin/env python3
"""
Small Sample Size Analysis and Documentation
æå°æ ·æœ¬é›†åˆ†æå’Œæ–‡æ¡£è¯´æ˜
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def analyze_small_samples():
    """åˆ†ææå°æ ·æœ¬æ•°æ®é›†"""
    print("ğŸ“Š Small Sample Size Analysis")
    print("=" * 50)
    
    # å°æ ·æœ¬æ•°æ®é›†ä¿¡æ¯
    small_datasets = {
        'phyto_long': {
            'samples': 82,
            'variables': 1,
            'type': 'Cross-sectional',
            'target': 'phytoplankton_abundance',
            'reason_excluded': 'Sample size below minimum threshold (N < 100)',
            'min_required': 100,
            'data_quality': 'High quality but insufficient quantity'
        },
        'phyto_wide': {
            'samples': 440,
            'variables': 46,
            'type': 'Cross-sectional', 
            'target': 'phytoplankton_abundance',
            'reason_excluded': 'High dimensionality with small sample (curse of dimensionality)',
            'min_required': 500,
            'data_quality': 'High dimensional species data'
        }
    }
    
    # åˆ›å»ºåˆ†ææŠ¥å‘Š
    analysis_results = []
    
    for dataset, info in small_datasets.items():
        # è®¡ç®—æ ·æœ¬-ç‰¹å¾æ¯”
        sample_feature_ratio = info['samples'] / info['variables']
        
        # åˆ¤æ–­æ˜¯å¦æ»¡è¶³æœ€å°è¦æ±‚
        meets_minimum = info['samples'] >= info['min_required']
        
        # ç»´åº¦è¯…å’’é£é™©
        curse_risk = "HIGH" if info['variables'] > info['samples'] else "LOW"
        
        analysis_results.append({
            'Dataset': dataset,
            'Samples': info['samples'],
            'Variables': info['variables'],
            'Sample/Feature Ratio': f"{sample_feature_ratio:.2f}",
            'Minimum Required': info['min_required'],
            'Meets Minimum': meets_minimum,
            'Curse of Dimensionality Risk': curse_risk,
            'Exclusion Reason': info['reason_excluded'],
            'Data Quality': info['data_quality'],
            'Recommendation': 'Descriptive statistics only' if not meets_minimum else 'Proceed with caution'
        })
        
        print(f"\nğŸ“‹ {dataset.upper()}")
        print(f"   Samples: {info['samples']}")
        print(f"   Variables: {info['variables']}")
        print(f"   Sample/Feature ratio: {sample_feature_ratio:.2f}")
        print(f"   Minimum required: {info['min_required']}")
        print(f"   Status: {'âœ… ADEQUATE' if meets_minimum else 'âŒ INSUFFICIENT'}")
        print(f"   Curse risk: {curse_risk}")
        print(f"   Reason: {info['reason_excluded']}")
    
    # ä¿å­˜åˆ†æç»“æœ
    analysis_df = pd.DataFrame(analysis_results)
    analysis_df.to_csv('tables/small_sample_analysis.csv', index=False)
    
    return analysis_df

def create_sample_size_guidelines():
    """åˆ›å»ºæ ·æœ¬é‡æŒ‡å¯¼åŸåˆ™"""
    print(f"\nğŸ“ Sample Size Guidelines")
    print("=" * 50)
    
    guidelines = {
        'Machine Learning Task': [
            'Simple Linear Regression',
            'Multiple Linear Regression', 
            'Random Forest',
            'Support Vector Regression',
            'Neural Networks (Simple)',
            'Deep Learning (LSTM)',
            'Cross-validation (5-fold)',
            'Bootstrap Confidence Intervals'
        ],
        'Minimum Sample Size': [
            30,
            '10 Ã— n_features',
            '5 Ã— n_features (min 100)',
            '10 Ã— n_features (min 200)',
            '100 Ã— n_features',
            '1000+ (time series)',
            '50 (10 per fold)',
            '30 (for stable estimates)'
        ],
        'Recommended Size': [
            100,
            '20 Ã— n_features',
            '10 Ã— n_features (min 500)',
            '20 Ã— n_features (min 1000)',
            '500 Ã— n_features',
            '5000+ (time series)',
            '500 (100 per fold)',
            '100 (for robust estimates)'
        ],
        'Our Datasets Status': [
            'Most datasets adequate',
            'Adequate for low-dim datasets',
            'Adequate for most datasets',
            'Adequate for most datasets',
            'Only large datasets adequate',
            'Only era5_daily adequate',
            'Most datasets adequate',
            'Most datasets adequate'
        ]
    }
    
    guidelines_df = pd.DataFrame(guidelines)
    guidelines_df.to_csv('tables/sample_size_guidelines.csv', index=False)
    
    print(guidelines_df.to_string(index=False))
    
    return guidelines_df

def generate_supplementary_text():
    """ç”Ÿæˆè¡¥å……ææ–™æ–‡æœ¬"""
    
    supplement_text = """
# Supplementary Material: Small Sample Size Datasets

## S1. Exclusion Criteria for Small Sample Datasets

Two datasets were excluded from the main model comparison analysis due to insufficient sample sizes:

### S1.1 phyto_long Dataset
- **Sample size**: N = 82
- **Variables**: 1 (phytoplankton abundance)
- **Exclusion reason**: Sample size below minimum threshold for reliable machine learning model training
- **Minimum required**: N â‰¥ 100 for basic cross-validation
- **Data quality**: High-quality measurements but insufficient quantity for robust model evaluation

### S1.2 phyto_wide Dataset  
- **Sample size**: N = 440
- **Variables**: 46 (species-level phytoplankton data)
- **Exclusion reason**: High dimensionality relative to sample size (curse of dimensionality)
- **Sample-to-feature ratio**: 9.6:1 (recommended minimum: 10:1 for tree-based models)
- **Minimum required**: N â‰¥ 500 for high-dimensional data
- **Data quality**: Comprehensive species-level data but dimensionality challenges

## S2. Sample Size Guidelines Applied

Following established machine learning best practices:

1. **Cross-validation requirements**: Minimum 50 samples (10 per fold for 5-fold CV)
2. **Tree-based models**: Minimum 5-10 samples per feature
3. **Statistical significance**: Minimum 30 samples for t-tests
4. **Bootstrap confidence intervals**: Minimum 30 samples for stable estimates

## S3. Descriptive Statistics for Excluded Datasets

### phyto_long (N=82):
- Mean phytoplankton abundance: [value]
- Standard deviation: [value]  
- Range: [min] - [max]
- Data collection period: [period]

### phyto_wide (N=440):
- Number of species detected: 46
- Mean species richness per sample: [value]
- Dominant species: [list top 5]
- Spatial coverage: [description]

## S4. Implications for Marine Ecosystem Modeling

The exclusion of small sample datasets highlights important considerations:

1. **Data collection priorities**: Emphasis on sustained, long-term monitoring
2. **Species-level modeling**: Requires substantial sample sizes for reliable predictions
3. **Community-level approaches**: May be more feasible with limited samples
4. **Temporal vs. spatial trade-offs**: Balance between temporal resolution and sample size

## S5. Recommendations for Future Studies

1. **Minimum sample sizes**: N â‰¥ 500 for high-dimensional biological data
2. **Data aggregation strategies**: Consider temporal or spatial pooling
3. **Dimensionality reduction**: Apply PCA or feature selection for high-dimensional data
4. **Ensemble approaches**: Combine multiple small datasets when appropriate

---

*Note: Complete raw data and metadata for excluded datasets are available in the supplementary data repository.*
"""
    
    # ä¿å­˜è¡¥å……ææ–™æ–‡æœ¬
    with open('supplementary_small_samples.md', 'w', encoding='utf-8') as f:
        f.write(supplement_text)
    
    print(f"âœ… Supplementary material text generated")
    print(f"   File: supplementary_small_samples.md")
    
    return supplement_text

def create_sample_size_visualization():
    """åˆ›å»ºæ ·æœ¬é‡å¯è§†åŒ–"""
    print(f"\nğŸ“Š Creating Sample Size Visualization")
    
    # æ‰€æœ‰æ•°æ®é›†çš„æ ·æœ¬é‡
    datasets = {
        'era5_daily': 102982,
        'cast': 21865, 
        'rolling_mean': 8855,
        'processed_seq': 8039,
        'cleaned_data': 7819,
        'biotoxin': 5076,
        'hydrographic': 4653,
        'phyto_wide': 440,
        'phyto_long': 82
    }
    
    # åˆ›å»ºå›¾è¡¨
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # å­å›¾1ï¼šæ‰€æœ‰æ•°æ®é›†ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰
    names = list(datasets.keys())
    sizes = list(datasets.values())
    colors = ['green' if s >= 1000 else 'orange' if s >= 500 else 'red' for s in sizes]
    
    bars1 = ax1.bar(names, sizes, color=colors, alpha=0.7)
    ax1.set_yscale('log')
    ax1.set_ylabel('Sample Size (log scale)')
    ax1.set_title('Dataset Sample Sizes')
    ax1.tick_params(axis='x', rotation=45)
    
    # æ·»åŠ é˜ˆå€¼çº¿
    ax1.axhline(y=1000, color='green', linestyle='--', alpha=0.7, label='Adequate (â‰¥1000)')
    ax1.axhline(y=500, color='orange', linestyle='--', alpha=0.7, label='Marginal (â‰¥500)')
    ax1.axhline(y=100, color='red', linestyle='--', alpha=0.7, label='Insufficient (<500)')
    ax1.legend()
    
    # å­å›¾2ï¼šæ ·æœ¬é‡åˆ†å¸ƒ
    size_categories = ['<100', '100-500', '500-1000', '1000-5000', '5000+']
    counts = [
        sum(1 for s in sizes if s < 100),
        sum(1 for s in sizes if 100 <= s < 500),
        sum(1 for s in sizes if 500 <= s < 1000),
        sum(1 for s in sizes if 1000 <= s < 5000),
        sum(1 for s in sizes if s >= 5000)
    ]
    
    colors2 = ['red', 'orange', 'yellow', 'lightgreen', 'green']
    ax2.pie(counts, labels=size_categories, colors=colors2, autopct='%1.0f%%')
    ax2.set_title('Sample Size Distribution')
    
    plt.tight_layout()
    plt.savefig('figures/sample_size_analysis.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… Sample size visualization saved: figures/sample_size_analysis.png")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“Š Small Sample Size Analysis and Documentation")
    print("=" * 60)
    
    # åˆ†æå°æ ·æœ¬æ•°æ®é›†
    analysis_df = analyze_small_samples()
    
    # åˆ›å»ºæ ·æœ¬é‡æŒ‡å¯¼åŸåˆ™
    guidelines_df = create_sample_size_guidelines()
    
    # ç”Ÿæˆè¡¥å……ææ–™æ–‡æœ¬
    supplement_text = generate_supplementary_text()
    
    # åˆ›å»ºå¯è§†åŒ–
    create_sample_size_visualization()
    
    print(f"\nğŸ¯ SMALL SAMPLE ANALYSIS COMPLETE")
    print("=" * 60)
    print("âœ… Small sample datasets analyzed")
    print("âœ… Exclusion criteria documented") 
    print("âœ… Sample size guidelines established")
    print("âœ… Supplementary material generated")
    print("âœ… Visualization created")
    
    print(f"\nğŸ“ Generated Files:")
    print(f"   - tables/small_sample_analysis.csv")
    print(f"   - tables/sample_size_guidelines.csv")
    print(f"   - supplementary_small_samples.md")
    print(f"   - figures/sample_size_analysis.png")

if __name__ == "__main__":
    main()
