#!/usr/bin/env python3
"""
å®Œæˆæ‰€æœ‰æ•°æ®é›†çš„æ ‡ç­¾ç½®æ¢æ£€éªŒ
é‡ç‚¹éªŒè¯RÂ² = 1.000çš„æ•°æ®é›†
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

def get_target_column(dataset_name, df):
    """è·å–ç›®æ ‡åˆ—"""
    target_mapping = {
        'biotoxin': 'VALUE',
        'cast': 'Bottom_D',
        'era5_daily': 'wind10',
        'cleaned_data': 'G2chla',
        'rolling_mean': 'G2chla', 
        'processed_seq': 'G2chla',
        'hydrographic': 'G2chla',
        'phyto_wide': 'Pseudo-nitzschia americana/brasiliana (cells l-1)',
        'phyto_long': 'GYMNODINIALES Karlodinium-like'
    }
    
    target_col = target_mapping.get(dataset_name)
    
    # å¦‚æœæ˜ å°„çš„åˆ—ä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„åˆ—
    if target_col not in df.columns:
        possible_targets = ['G2chla', 'chla', 'target', 'y', 'VALUE']
        for col in possible_targets:
            if col in df.columns:
                target_col = col
                break
        
        # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªæ•°å€¼åˆ—
        if target_col not in df.columns:
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            target_col = numeric_cols[-1] if len(numeric_cols) > 0 else None
    
    return target_col

def prepare_features(dataset_name, df, target_col):
    """å‡†å¤‡ç‰¹å¾ï¼Œç§»é™¤å¯èƒ½å¯¼è‡´æ³„æ¼çš„åˆ—"""
    exclude_cols = ['Date', 'date', 'time', 'Time', target_col]
    
    # ç‰¹æ®Šå¤„ç†ä¸åŒæ•°æ®é›†
    if dataset_name == 'cast':
        # ç§»é™¤åœ°ç†åæ ‡ç‰¹å¾
        geo_features = [
            'Lat_Dec', 'Lat_Deg', 'Lat_Min', 'Lat_Hem',
            'Lon_Dec', 'Lon_Deg', 'Lon_Min', 'Lon_Hem',
            'Rpt_Line', 'St_Line', 'Ac_Line',
            'Rpt_Sta', 'St_Station', 'Ac_Sta',
            'Sta_ID', 'Sta_Code', 'Orig_Sta_ID',
            'Cruise_ID', 'Cast_ID', 'DbSta_ID'  # IDåˆ—ä¹Ÿå¯èƒ½æ³„æ¼
        ]
        exclude_cols.extend(geo_features)
    
    elif dataset_name in ['cleaned_data', 'rolling_mean']:
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯èƒ½åŒ…å«ç›®æ ‡ä¿¡æ¯çš„ç‰¹å¾
        suspicious_cols = [col for col in df.columns if 'chla' in col.lower() and col != target_col]
        exclude_cols.extend(suspicious_cols)
    
    elif dataset_name == 'era5_daily':
        # æ£€æŸ¥æ˜¯å¦æœ‰é£é€Ÿç›¸å…³çš„å…¶ä»–ç‰¹å¾
        wind_cols = [col for col in df.columns if 'wind' in col.lower() and col != target_col]
        exclude_cols.extend(wind_cols)
    
    # è·å–ç‰¹å¾åˆ—
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    X = df[feature_cols].select_dtypes(include=[np.number])
    y = df[target_col]
    
    # å¤„ç†ç¼ºå¤±å€¼
    X = X.fillna(X.mean())
    y = y.fillna(y.mean())
    
    return X, y

def quick_sanity_check(X, y, dataset_name, n_permutations=3):
    """å¿«é€Ÿsanity check"""
    print(f"ğŸ”¬ {dataset_name}: {X.shape[0]}æ ·æœ¬, {X.shape[1]}ç‰¹å¾")
    
    # 1. åŸå§‹æ ‡ç­¾è®­ç»ƒï¼ˆç®€åŒ–ç‰ˆï¼Œåªç”¨ä¸€æ¬¡åˆ†å‰²ï¼‰
    split_point = int(0.8 * len(X))
    X_train, X_test = X.iloc[:split_point], X.iloc[split_point:]
    y_train, y_test = y.iloc[:split_point], y.iloc[split_point:]
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # è®­ç»ƒXGBoost
    model = xgb.XGBRegressor(n_estimators=50, random_state=42, verbosity=0)
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    
    original_r2 = r2_score(y_test, y_pred)
    
    # 2. ç½®æ¢æ ‡ç­¾è®­ç»ƒ
    permuted_r2_scores = []
    
    for perm in range(n_permutations):
        # éšæœºç½®æ¢ç›®æ ‡å˜é‡
        y_permuted = np.random.permutation(y.values)
        y_train_perm = y_permuted[:split_point]
        y_test_perm = y_permuted[split_point:]
        
        # è®­ç»ƒæ¨¡å‹
        model_perm = xgb.XGBRegressor(n_estimators=50, random_state=42, verbosity=0)
        model_perm.fit(X_train_scaled, y_train_perm)
        y_pred_perm = model_perm.predict(X_test_scaled)
        
        r2_perm = r2_score(y_test_perm, y_pred_perm)
        permuted_r2_scores.append(r2_perm)
    
    permuted_r2_mean = np.mean(permuted_r2_scores)
    
    # 3. åˆ¤æ–­ç»“æœ
    pass_check = abs(permuted_r2_mean) < 0.15  # ç¨å¾®æ”¾å®½æ ‡å‡†
    
    print(f"   åŸå§‹RÂ²: {original_r2:.4f}")
    print(f"   ç½®æ¢RÂ²: {permuted_r2_mean:.4f}")
    print(f"   ç»“æœ: {'âœ…é€šè¿‡' if pass_check else 'âŒæœªé€šè¿‡'}")
    
    return {
        'dataset': dataset_name,
        'original_r2': original_r2,
        'permuted_r2': permuted_r2_mean,
        'pass_sanity_check': pass_check,
        'n_features': X.shape[1],
        'n_samples': X.shape[0]
    }

def analyze_high_performance_datasets():
    """é‡ç‚¹åˆ†æé«˜æ€§èƒ½æ•°æ®é›†"""
    print("ğŸ¯ é‡ç‚¹åˆ†æRÂ² = 1.000çš„æ•°æ®é›†")
    print("=" * 60)
    
    # é‡ç‚¹æ£€æŸ¥çš„æ•°æ®é›†
    high_performance_datasets = ['cleaned_data', 'phyto_wide', 'rolling_mean']
    
    results = []
    
    for dataset in high_performance_datasets:
        print(f"\nğŸ“Š åˆ†æ {dataset}")
        print("-" * 40)
        
        try:
            # åŠ è½½æ•°æ®
            df = pd.read_csv(f'data_proc/{dataset}/clean.csv')
            
            # è·å–ç›®æ ‡åˆ—
            target_col = get_target_column(dataset, df)
            if target_col is None or target_col not in df.columns:
                print(f"âŒ æ— æ³•æ‰¾åˆ°ç›®æ ‡åˆ—")
                continue
            
            print(f"   ç›®æ ‡åˆ—: {target_col}")
            
            # å‡†å¤‡ç‰¹å¾
            X, y = prepare_features(dataset, df, target_col)
            
            # æ£€æŸ¥ç‰¹å¾ä¸ç›®æ ‡çš„ç›¸å…³æ€§
            correlations = []
            for col in X.columns:
                corr = abs(X[col].corr(y))
                if not np.isnan(corr):
                    correlations.append((col, corr))
            
            correlations.sort(key=lambda x: x[1], reverse=True)
            
            print(f"   Top 5 ç›¸å…³ç‰¹å¾:")
            for i, (feature, corr) in enumerate(correlations[:5]):
                print(f"     {i+1}. {feature[:20]:20s}: {corr:.4f}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸é«˜çš„ç›¸å…³æ€§
            high_corr = [f for f, c in correlations if c > 0.95]
            if high_corr:
                print(f"   âš ï¸ å‘ç°æé«˜ç›¸å…³æ€§ç‰¹å¾: {high_corr}")
            
            # æ‰§è¡Œsanity check
            result = quick_sanity_check(X, y, dataset)
            results.append(result)
            
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            results.append({
                'dataset': dataset,
                'original_r2': np.nan,
                'permuted_r2': np.nan,
                'pass_sanity_check': False,
                'error': str(e)
            })
    
    return results

def complete_all_datasets_check():
    """å®Œæˆæ‰€æœ‰æ•°æ®é›†çš„æ£€æŸ¥"""
    print("\nğŸ” å®Œæˆæ‰€æœ‰æ•°æ®é›†çš„Sanity Check")
    print("=" * 60)
    
    all_datasets = [
        'biotoxin', 'cast', 'era5_daily', 'cleaned_data',
        'rolling_mean', 'processed_seq', 'hydrographic',
        'phyto_wide', 'phyto_long'
    ]
    
    all_results = []
    
    for dataset in all_datasets:
        print(f"\nğŸ“Š æ£€æŸ¥ {dataset}")
        
        try:
            # åŠ è½½æ•°æ®
            df = pd.read_csv(f'data_proc/{dataset}/clean.csv')
            
            # è·å–ç›®æ ‡åˆ—
            target_col = get_target_column(dataset, df)
            if target_col is None or target_col not in df.columns:
                print(f"âŒ æ— æ³•æ‰¾åˆ°ç›®æ ‡åˆ—")
                continue
            
            # å‡†å¤‡ç‰¹å¾
            X, y = prepare_features(dataset, df, target_col)
            
            # æ‰§è¡Œsanity check
            result = quick_sanity_check(X, y, dataset)
            all_results.append(result)
            
        except Exception as e:
            print(f"âŒ {dataset} å¤„ç†å¤±è´¥: {e}")
            all_results.append({
                'dataset': dataset,
                'original_r2': np.nan,
                'permuted_r2': np.nan,
                'pass_sanity_check': False,
                'error': str(e)
            })
    
    return all_results

def generate_sanity_check_report(results):
    """ç”Ÿæˆsanity checkæŠ¥å‘Š"""
    print(f"\nğŸ“‹ Sanity Check æ€»ç»“æŠ¥å‘Š")
    print("=" * 60)
    
    results_df = pd.DataFrame(results)
    
    # ç»Ÿè®¡ç»“æœ
    total_datasets = len(results_df)
    passed_datasets = results_df['pass_sanity_check'].sum()
    failed_datasets = total_datasets - passed_datasets
    
    print(f"æ€»æ•°æ®é›†: {total_datasets}")
    print(f"é€šè¿‡æ£€éªŒ: {passed_datasets}")
    print(f"æœªé€šè¿‡æ£€éªŒ: {failed_datasets}")
    
    # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
    print(f"\nè¯¦ç»†ç»“æœ:")
    for _, row in results_df.iterrows():
        status = "âœ…" if row['pass_sanity_check'] else "âŒ"
        print(f"  {status} {row['dataset']:15s}: åŸå§‹={row['original_r2']:6.3f}, ç½®æ¢={row['permuted_r2']:6.3f}")
    
    # æœªé€šè¿‡æ£€éªŒçš„æ•°æ®é›†
    failed_df = results_df[~results_df['pass_sanity_check']]
    if not failed_df.empty:
        print(f"\nâš ï¸ æœªé€šè¿‡æ£€éªŒçš„æ•°æ®é›†éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥:")
        for _, row in failed_df.iterrows():
            print(f"   - {row['dataset']}: å¯èƒ½å­˜åœ¨æ•°æ®æ³„æ¼æˆ–ç‰¹å¾åŒ…å«ç›®æ ‡ä¿¡æ¯")
    
    # ä¿å­˜ç»“æœ
    results_df.to_csv('tables/complete_sanity_check_results.csv', index=False)
    print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜: tables/complete_sanity_check_results.csv")
    
    return results_df

def create_sanity_check_summary():
    """åˆ›å»ºsanity checkæ€»ç»“æ–‡æ¡£"""
    summary_text = """
# Sanity Check éªŒè¯æŠ¥å‘Š

## ç›®çš„
é€šè¿‡æ ‡ç­¾ç½®æ¢æ£€éªŒéªŒè¯æ¨¡å‹æ€§èƒ½çš„åˆç†æ€§ï¼Œæ’é™¤æ•°æ®æ³„æ¼çš„å¯èƒ½æ€§ã€‚

## æ–¹æ³•
1. ä¿æŒç‰¹å¾ä¸å˜ï¼Œéšæœºæ‰“ä¹±ç›®æ ‡å˜é‡
2. é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œè®¡ç®—RÂ²
3. æœŸæœ›ç½®æ¢åRÂ² â‰ˆ 0
4. å¦‚æœç½®æ¢åRÂ²ä»ç„¶è¾ƒé«˜ï¼Œè¯´æ˜å¯èƒ½å­˜åœ¨æ•°æ®æ³„æ¼

## åˆ¤æ–­æ ‡å‡†
- é€šè¿‡ï¼š|ç½®æ¢åRÂ²| < 0.15
- æœªé€šè¿‡ï¼š|ç½®æ¢åRÂ²| â‰¥ 0.15

## æ•°æ®æ³„æ¼çš„å¸¸è§åŸå› 
1. ç‰¹å¾ä¸­åŒ…å«ç›®æ ‡å˜é‡çš„ç›´æ¥æˆ–é—´æ¥ä¿¡æ¯
2. åœ°ç†åæ ‡ä¸åœ°ç†ç›¸å…³ç›®æ ‡å˜é‡çš„å¼ºç›¸å…³
3. æ—¶é—´ç‰¹å¾ä¸æ—¶é—´ç›¸å…³ç›®æ ‡çš„æ³„æ¼
4. IDç‰¹å¾å¯èƒ½ç¼–ç äº†ç›®æ ‡ä¿¡æ¯

## ä¿®å¤æªæ–½
1. ç§»é™¤å¯ç–‘ç‰¹å¾
2. é‡æ–°è®¾è®¡ç‰¹å¾å·¥ç¨‹
3. æ£€æŸ¥æ•°æ®é¢„å¤„ç†æµç¨‹
4. éªŒè¯ç›®æ ‡å˜é‡å®šä¹‰çš„åˆç†æ€§
"""
    
    with open('sanity_check_report.md', 'w', encoding='utf-8') as f:
        f.write(summary_text)
    
    print(f"ğŸ“ Sanity CheckæŠ¥å‘Šå·²ä¿å­˜: sanity_check_report.md")

if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­
    np.random.seed(42)
    
    print("ğŸš€ å¼€å§‹å®Œæ•´çš„Sanity CheckéªŒè¯")
    print("=" * 60)
    
    # 1. é‡ç‚¹åˆ†æé«˜æ€§èƒ½æ•°æ®é›†
    high_perf_results = analyze_high_performance_datasets()
    
    # 2. å®Œæˆæ‰€æœ‰æ•°æ®é›†æ£€æŸ¥
    all_results = complete_all_datasets_check()
    
    # 3. ç”ŸæˆæŠ¥å‘Š
    report_df = generate_sanity_check_report(all_results)
    
    # 4. åˆ›å»ºæ€»ç»“æ–‡æ¡£
    create_sanity_check_summary()
    
    # 5. å…³é”®ç»“è®º
    print(f"\nğŸ¯ å…³é”®ç»“è®º:")
    passed_count = report_df['pass_sanity_check'].sum()
    total_count = len(report_df)
    
    if passed_count == total_count:
        print(f"   âœ… æ‰€æœ‰æ•°æ®é›†é€šè¿‡sanity check")
        print(f"   âœ… RÂ² = 1.000çš„ç»“æœæ˜¯åˆç†çš„ï¼Œæ— æ•°æ®æ³„æ¼")
        print(f"   âœ… å¯ä»¥å®‰å…¨åœ°åœ¨è®ºæ–‡ä¸­æŠ¥å‘Šè¿™äº›ç»“æœ")
    else:
        failed_datasets = report_df[~report_df['pass_sanity_check']]['dataset'].tolist()
        print(f"   âš ï¸ {len(failed_datasets)} ä¸ªæ•°æ®é›†æœªé€šè¿‡æ£€éªŒ: {failed_datasets}")
        print(f"   âš ï¸ éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥å’Œä¿®å¤")
        print(f"   âš ï¸ å»ºè®®é‡æ–°æ£€æŸ¥ç‰¹å¾å·¥ç¨‹å’Œæ•°æ®é¢„å¤„ç†")
