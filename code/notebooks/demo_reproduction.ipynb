{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marine ML Benchmark: Demo Reproduction Notebook\n",
    "\n",
    "This notebook demonstrates how to reproduce the key results from the Marine ML Benchmark study.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook will walk you through:\n",
    "1. Loading and exploring the processed datasets\n",
    "2. Training models on sample data\n",
    "3. Evaluating model performance\n",
    "4. Generating visualizations\n",
    "5. Reproducing key findings\n",
    "\n",
    "**Note**: This is a demonstration notebook. For full reproduction, use the provided shell scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration\n",
    "\n",
    "Let's start by exploring the processed datasets to understand their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset characteristics\n",
    "data_dir = Path('../../data/processed')\n",
    "tables_dir = Path('../../outputs/tables')\n",
    "\n",
    "# Load dataset summary\n",
    "if (tables_dir / 'final_table1_dataset_characteristics.csv').exists():\n",
    "    dataset_chars = pd.read_csv(tables_dir / 'final_table1_dataset_characteristics.csv')\n",
    "    print(\"ğŸ“Š Dataset Characteristics:\")\n",
    "    print(dataset_chars.to_string(index=False))\n",
    "else:\n",
    "    print(\"âš ï¸  Dataset characteristics table not found. Run the full pipeline first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore a sample dataset\n",
    "sample_dataset = 'cleaned_data'\n",
    "sample_path = data_dir / sample_dataset / 'clean.csv'\n",
    "\n",
    "if sample_path.exists():\n",
    "    df_sample = pd.read_csv(sample_path)\n",
    "    \n",
    "    print(f\"ğŸ“ˆ Sample Dataset: {sample_dataset}\")\n",
    "    print(f\"Shape: {df_sample.shape}\")\n",
    "    print(f\"Columns: {list(df_sample.columns[:10])}{'...' if len(df_sample.columns) > 10 else ''}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df_sample.head())\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nğŸ“Š Basic Statistics:\")\n",
    "    print(df_sample.describe())\n",
    "else:\n",
    "    print(f\"âš ï¸  Sample dataset {sample_dataset} not found.\")\n",
    "    print(\"Available datasets:\")\n",
    "    for dataset_dir in data_dir.glob('*/'):\n",
    "        if (dataset_dir / 'clean.csv').exists():\n",
    "            print(f\"  - {dataset_dir.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training Demo\n",
    "\n",
    "Let's demonstrate model training on a sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "if 'df_sample' in locals() and not df_sample.empty:\n",
    "    # Identify target column (assuming it's the last numerical column or contains 'chla')\n",
    "    numerical_cols = df_sample.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Try to find chlorophyll-a related column\n",
    "    target_candidates = [col for col in numerical_cols if 'chla' in col.lower() or 'chlorophyll' in col.lower()]\n",
    "    \n",
    "    if target_candidates:\n",
    "        target_col = target_candidates[0]\n",
    "    else:\n",
    "        target_col = numerical_cols[-1]  # Use last numerical column\n",
    "    \n",
    "    print(f\"ğŸ¯ Target variable: {target_col}\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    feature_cols = [col for col in numerical_cols if col != target_col]\n",
    "    X = df_sample[feature_cols].fillna(df_sample[feature_cols].mean())\n",
    "    y = df_sample[target_col].fillna(df_sample[target_col].mean())\n",
    "    \n",
    "    print(f\"ğŸ“Š Features: {len(feature_cols)} columns\")\n",
    "    print(f\"ğŸ“Š Samples: {len(X)} observations\")\n",
    "    print(f\"ğŸ“Š Target range: [{y.min():.4f}, {y.max():.4f}]\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… Data prepared for modeling\")\n",
    "    print(f\"   Training samples: {len(X_train)}\")\n",
    "    print(f\"   Test samples: {len(X_test)}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No sample data available for modeling demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models\n",
    "if 'X_train' in locals():\n",
    "    models = {\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42, verbosity=0)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"ğŸ¤– Training models...\")\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n   Training {name}...\")\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        test_rmse = mean_squared_error(y_test, y_pred_test, squared=False)\n",
    "        \n",
    "        results[name] = {\n",
    "            'Train RÂ²': train_r2,\n",
    "            'Test RÂ²': test_r2,\n",
    "            'Test MAE': test_mae,\n",
    "            'Test RMSE': test_rmse\n",
    "        }\n",
    "        \n",
    "        print(f\"     Train RÂ²: {train_r2:.4f}\")\n",
    "        print(f\"     Test RÂ²: {test_r2:.4f}\")\n",
    "        print(f\"     Test MAE: {test_mae:.4f}\")\n",
    "    \n",
    "    # Display results\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    print(\"\\nğŸ“Š Model Performance Summary:\")\n",
    "    print(results_df.round(4))\n",
    "else:\n",
    "    print(\"âš ï¸  Cannot train models without prepared data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross-Validation Demo\n",
    "\n",
    "Demonstrate cross-validation for more robust performance estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation demonstration\n",
    "if 'X' in locals() and 'y' in locals():\n",
    "    print(\"ğŸ”„ Cross-Validation Analysis\")\n",
    "    \n",
    "    # Use TimeSeriesSplit for demonstration (even if not time series)\n",
    "    cv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    cv_results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n   {name} Cross-Validation:\")\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_scores = cross_val_score(model, X, y, cv=cv, scoring='r2')\n",
    "        \n",
    "        cv_results[name] = {\n",
    "            'Mean RÂ²': cv_scores.mean(),\n",
    "            'Std RÂ²': cv_scores.std(),\n",
    "            'Min RÂ²': cv_scores.min(),\n",
    "            'Max RÂ²': cv_scores.max()\n",
    "        }\n",
    "        \n",
    "        print(f\"     Mean RÂ²: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "        print(f\"     Range: [{cv_scores.min():.4f}, {cv_scores.max():.4f}]\")\n",
    "    \n",
    "    # Display CV results\n",
    "    cv_results_df = pd.DataFrame(cv_results).T\n",
    "    print(\"\\nğŸ“Š Cross-Validation Summary:\")\n",
    "    print(cv_results_df.round(4))\n",
    "else:\n",
    "    print(\"âš ï¸  Cannot perform cross-validation without data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization Demo\n",
    "\n",
    "Create some basic visualizations to understand model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of results\n",
    "if 'results_df' in locals():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: Model performance comparison\n",
    "    results_df[['Test RÂ²', 'Test MAE']].plot(kind='bar', ax=axes[0])\n",
    "    axes[0].set_title('Model Performance Comparison')\n",
    "    axes[0].set_ylabel('Score')\n",
    "    axes[0].legend()\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 2: Prediction vs Actual (for best model)\n",
    "    if 'models' in locals() and 'X_test' in locals():\n",
    "        best_model_name = results_df['Test RÂ²'].idxmax()\n",
    "        best_model = models[best_model_name]\n",
    "        y_pred_best = best_model.predict(X_test)\n",
    "        \n",
    "        axes[1].scatter(y_test, y_pred_best, alpha=0.6)\n",
    "        axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "        axes[1].set_xlabel('Actual Values')\n",
    "        axes[1].set_ylabel('Predicted Values')\n",
    "        axes[1].set_title(f'Predictions vs Actual ({best_model_name})')\n",
    "        \n",
    "        # Add RÂ² to plot\n",
    "        r2_best = r2_score(y_test, y_pred_best)\n",
    "        axes[1].text(0.05, 0.95, f'RÂ² = {r2_best:.4f}', \n",
    "                    transform=axes[1].transAxes, fontsize=12,\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ… Visualizations generated\")\n",
    "else:\n",
    "    print(\"âš ï¸  No results available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Display Paper Results\n",
    "\n",
    "Load and display the actual results from the full benchmark study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load paper results\n",
    "print(\"ğŸ“Š Loading Paper Results...\")\n",
    "\n",
    "# Load performance results\n",
    "if (tables_dir / 'final_table2_model_performance.csv').exists():\n",
    "    performance_results = pd.read_csv(tables_dir / 'final_table2_model_performance.csv')\n",
    "    \n",
    "    print(\"\\nğŸ† Model Performance Results (Sample):\")\n",
    "    print(performance_results.head(10))\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nğŸ“ˆ Performance Summary by Model:\")\n",
    "    model_summary = performance_results.groupby('Model')['RÂ²'].agg(['mean', 'std', 'count']).round(4)\n",
    "    print(model_summary)\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  Performance results not found. Run the full pipeline first.\")\n",
    "\n",
    "# Load best performance results\n",
    "if (tables_dir / 'final_table3_best_performance.csv').exists():\n",
    "    best_results = pd.read_csv(tables_dir / 'final_table3_best_performance.csv')\n",
    "    \n",
    "    print(\"\\nğŸ¥‡ Best Model per Dataset:\")\n",
    "    print(best_results.to_string(index=False))\n",
    "else:\n",
    "    print(\"âš ï¸  Best performance results not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary visualization of paper results\n",
    "if 'performance_results' in locals():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Plot 1: Performance by model\n",
    "    model_perf = performance_results.groupby('Model')['RÂ²'].mean().sort_values(ascending=False)\n",
    "    model_perf.plot(kind='bar', ax=axes[0,0], color='skyblue')\n",
    "    axes[0,0].set_title('Average RÂ² by Model')\n",
    "    axes[0,0].set_ylabel('RÂ² Score')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 2: Performance by dataset\n",
    "    dataset_perf = performance_results.groupby('Dataset')['RÂ²'].mean().sort_values(ascending=False)\n",
    "    dataset_perf.plot(kind='bar', ax=axes[0,1], color='lightcoral')\n",
    "    axes[0,1].set_title('Average RÂ² by Dataset')\n",
    "    axes[0,1].set_ylabel('RÂ² Score')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 3: Performance distribution\n",
    "    performance_results.boxplot(column='RÂ²', by='Model', ax=axes[1,0])\n",
    "    axes[1,0].set_title('RÂ² Distribution by Model')\n",
    "    axes[1,0].set_xlabel('Model')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 4: Model robustness (std vs mean)\n",
    "    model_stats = performance_results.groupby('Model')['RÂ²'].agg(['mean', 'std'])\n",
    "    axes[1,1].scatter(model_stats['mean'], model_stats['std'])\n",
    "    for model, (mean_r2, std_r2) in model_stats.iterrows():\n",
    "        axes[1,1].annotate(model, (mean_r2, std_r2), xytext=(5, 5), \n",
    "                          textcoords='offset points', fontsize=10)\n",
    "    axes[1,1].set_xlabel('Mean RÂ²')\n",
    "    axes[1,1].set_ylabel('Standard Deviation RÂ²')\n",
    "    axes[1,1].set_title('Model Robustness (Lower std = More robust)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Paper results visualization completed\")\n",
    "else:\n",
    "    print(\"âš ï¸  Cannot create paper results visualization without data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Findings Summary\n",
    "\n",
    "Summarize the key findings from the benchmark study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” KEY FINDINGS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'performance_results' in locals():\n",
    "    # Calculate key statistics\n",
    "    model_stats = performance_results.groupby('Model')['RÂ²'].agg(['mean', 'std', 'count'])\n",
    "    best_model = model_stats['mean'].idxmax()\n",
    "    best_mean_r2 = model_stats.loc[best_model, 'mean']\n",
    "    best_std_r2 = model_stats.loc[best_model, 'std']\n",
    "    \n",
    "    dataset_stats = performance_results.groupby('Dataset')['RÂ²'].agg(['mean', 'max', 'min'])\n",
    "    easiest_dataset = dataset_stats['mean'].idxmax()\n",
    "    hardest_dataset = dataset_stats['mean'].idxmin()\n",
    "    \n",
    "    print(f\"ğŸ† BEST OVERALL MODEL: {best_model}\")\n",
    "    print(f\"   Mean RÂ²: {best_mean_r2:.4f} Â± {best_std_r2:.4f}\")\n",
    "    print(f\"   Evaluated on: {model_stats.loc[best_model, 'count']} datasets\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š DATASET DIFFICULTY:\")\n",
    "    print(f\"   Easiest: {easiest_dataset} (RÂ² = {dataset_stats.loc[easiest_dataset, 'mean']:.4f})\")\n",
    "    print(f\"   Hardest: {hardest_dataset} (RÂ² = {dataset_stats.loc[hardest_dataset, 'mean']:.4f})\")\n",
    "    \n",
    "    # Model ranking\n",
    "    print(f\"\\nğŸ¥‡ MODEL RANKING (by mean RÂ²):\")\n",
    "    for i, (model, stats) in enumerate(model_stats.sort_values('mean', ascending=False).iterrows(), 1):\n",
    "        print(f\"   {i}. {model}: {stats['mean']:.4f} Â± {stats['std']:.4f}\")\n",
    "    \n",
    "    # Success rate analysis\n",
    "    success_threshold = 0.5\n",
    "    success_rate = (performance_results['RÂ²'] > success_threshold).mean()\n",
    "    print(f\"\\nâœ… SUCCESS RATE (RÂ² > {success_threshold}): {success_rate:.1%}\")\n",
    "    \n",
    "    # Deep learning analysis\n",
    "    dl_models = ['LSTM', 'TRANSFORMER']\n",
    "    dl_results = performance_results[performance_results['Model'].isin(dl_models)]\n",
    "    if not dl_results.empty:\n",
    "        dl_datasets = dl_results['Dataset'].nunique()\n",
    "        total_datasets = performance_results['Dataset'].nunique()\n",
    "        print(f\"\\nğŸ§  DEEP LEARNING APPLICABILITY: {dl_datasets}/{total_datasets} datasets\")\n",
    "        print(f\"   Limited by data structure requirements\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  Full results not available. Key findings from paper:\")\n",
    "    print(\"\\nğŸ† BEST OVERALL MODEL: XGBoost\")\n",
    "    print(\"   Mean RÂ²: 0.823 Â± 0.320\")\n",
    "    print(\"   Most robust across heterogeneous datasets\")\n",
    "    \n",
    "    print(\"\\nğŸ“Š DATASET DIFFICULTY:\")\n",
    "    print(\"   Easy (RÂ² > 0.9): 6 datasets\")\n",
    "    print(\"   Medium (0.6 < RÂ² < 0.9): 2 datasets\")\n",
    "    print(\"   Hard (RÂ² < 0.3): 1 dataset (biotoxin)\")\n",
    "    \n",
    "    print(\"\\nğŸ§  DEEP LEARNING LIMITATION:\")\n",
    "    print(\"   Only applicable to 5/9 datasets\")\n",
    "    print(\"   Requires sufficient temporal structure\")\n",
    "\n",
    "print(\"\\nğŸ’¡ PRACTICAL IMPLICATIONS:\")\n",
    "print(\"   â€¢ Data quality more important than quantity\")\n",
    "print(\"   â€¢ XGBoost recommended for most oceanographic datasets\")\n",
    "print(\"   â€¢ Deep learning requires careful data structure assessment\")\n",
    "print(\"   â€¢ Cross-dataset validation essential for robustness\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"âœ¨ Demo completed! For full reproduction, run the pipeline scripts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This notebook provided a demonstration of the key concepts and results. For complete reproduction:\n",
    "\n",
    "1. **Quick Test**: Run `bash code/scripts/run_quick_test.sh` to verify installation\n",
    "2. **Full Pipeline**: Run `bash code/scripts/run_full_pipeline.sh` to reproduce all results\n",
    "3. **Custom Analysis**: Modify the code to explore specific aspects of interest\n",
    "4. **Extension**: Add new datasets or models following the established framework\n",
    "\n",
    "## References\n",
    "\n",
    "- Paper: \"Cross-Dataset Robustness of Machine Learning Models for Chlorophyll-a Prediction: A Comprehensive Benchmark Study\"\n",
    "- Repository: https://github.com/[username]/marine-ml-benchmark\n",
    "- Documentation: See `docs/` directory for detailed methodology\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook is part of the Marine ML Benchmark project. For questions or issues, please refer to the GitHub repository.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
